{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b7fef85-8a9d-4561-9ca9-279d2b9aa862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 09:37:40.209501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 09:37:40.218811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 09:37:40.219235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "467e46e1-6078-4599-b55c-12ba94b138d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "name='j'\n",
    "dataset='app_store'\n",
    "encoding='sbert'\n",
    "al_steps=100\n",
    "sample_size_per_step=1\n",
    "save_and_repeat=1\n",
    "query_strategy='unc'\n",
    "random=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1e43c1d-9121-4ec2-ab8a-00136bd85b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "cwd = os.getcwd()\n",
    "sys.path.append(cwd + '/../.') \n",
    "\n",
    "import math\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from src.EvaluationLoop import EvaluationLoop\n",
    "from src.training_loop2 import QueryStrategy\n",
    "from data.datasets import Datasets, Encodings\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TFBertForSequenceClassification, TFTrainer, TFTrainingArguments, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a10d6f8-3a17-4730-a9e0-9850a4c13992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############\n",
      "False\n",
      "#############\n",
      "load index ... \n",
      "../eval_proxy/res/app_store_sbert_unc_j_1_100_1/used_training_index.npy\n",
      "load data...\n",
      "Model #0 ...\n",
      "tokenize ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 09:37:56.207272: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-02 09:37:56.208509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 09:37:56.208986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 09:37:56.209366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 09:37:57.069946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 09:37:57.070405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 09:37:57.070737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 09:37:57.071072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29064 MB memory:  -> device: 0, name: GRID V100S-32Q, pci bus id: 0000:06:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "    name_postfix = \"\"\n",
    "    \n",
    "    print('#############')\n",
    "    print(random)\n",
    "    print('#############')\n",
    "    \n",
    "    if random:\n",
    "        print('Random query strategy is used. Selection is ignored.')\n",
    "        name_postfix = \"_rnd\"\n",
    "        query_strategy = QueryStrategy.unc\n",
    "    \n",
    "    print('load index ... ')\n",
    "    used_training_index_dir = f'../eval_proxy/res/{dataset}_{encoding}_{str(query_strategy)}_{name}{name_postfix}_{sample_size_per_step}_{al_steps}_{save_and_repeat}/used_training_index.npy'\n",
    "    \n",
    "    print(used_training_index_dir)\n",
    "    used_training_index = np.load(used_training_index_dir, allow_pickle=True)\n",
    "    \n",
    "    print('load data...')\n",
    "    data_X = np.load(f'../data/datasets/{dataset}_X.npy')\n",
    "    data_y = np.load(f'../data/datasets/{dataset}_y.npy')\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
    "    sss.get_n_splits(data_X, data_y)\n",
    "    i = 0\n",
    "    for train_index, test_index in sss.split(data_X, data_y):\n",
    "        print(f'Model #{i} ...')\n",
    "        X_train, X_test = data_X[train_index], data_X[test_index]\n",
    "        y_train, y_test = data_y[train_index], data_y[test_index]\n",
    "        \n",
    "        idx = used_training_index[i][-1]\n",
    "        \n",
    "        X_used = X_train[idx]\n",
    "        y_used = y_train[idx]\n",
    "        \n",
    "        X_bert_train, X_bert_eval, y_bert_train, y_bert_eval = train_test_split(X_used, y_used, test_size=0.10, random_state=42, stratify=y_used)\n",
    "        \n",
    "        del X_used\n",
    "        del y_used\n",
    "        del X_train\n",
    "        del y_train\n",
    "        \n",
    "        print('tokenize ...')\n",
    "        tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        train_encodings = tokenizer(X_bert_train.tolist(), truncation=True, padding=True)\n",
    "        val_encodings = tokenizer(X_bert_eval.tolist(), truncation=True, padding=True)\n",
    "        test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True)\n",
    "        \n",
    "        del X_bert_train\n",
    "        del X_bert_eval\n",
    "        del X_test\n",
    "        del tokenizer\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(train_encodings),\n",
    "            y_bert_train\n",
    "        ))\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(val_encodings),\n",
    "            y_bert_eval\n",
    "        ))\n",
    "        #test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        #    dict(test_encodings),\n",
    "        #    y_test\n",
    "        #))\n",
    "        \n",
    "        del train_encodings\n",
    "        del val_encodings\n",
    "        del test_encodings\n",
    "        \n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe0e7d9-0959-480a-a958-a0ef78b0c227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load BERT model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer_tf.py:109: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/master/examples/tensorflow\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "        training_args = TFTrainingArguments(\n",
    "                    evaluation_strategy= \"steps\",\n",
    "                    output_dir='./bert_models',  \n",
    "                    num_train_epochs=10,              # total number of training epochs\n",
    "                    per_device_train_batch_size=16,  # batch size per device during training\n",
    "                    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "                    weight_decay=0.01,               # strength of weight decay\n",
    "                    warmup_steps=math.ceil(len(y_bert_train) / 16),\n",
    "                    logging_dir='./logs',         # directory for storing logs\n",
    "                    logging_steps=2000,\n",
    "                    learning_rate= 5e-5,\n",
    "                    save_total_limit = 1,\n",
    "                    #load_best_model_at_end= True,\n",
    "                )\n",
    "        \n",
    "        print('Load BERT model ...')\n",
    "        \n",
    "        with training_args.strategy.scope():\n",
    "            model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "\n",
    "        trainer = TFTrainer(\n",
    "            model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "            args=training_args,                  # training arguments, defined above\n",
    "            train_dataset=train_dataset,         # training dataset\n",
    "            eval_dataset=val_dataset             # evaluation dataset\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be39169-8f60-4118-a175-99c722962444",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df7b0cd-00b1-4ac6-9589-7391f2b2f592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c75192-096c-4d59-a18c-f30b4e6bda6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "        training_args = TFTrainingArguments(\n",
    "                    evaluation_strategy= \"steps\",\n",
    "                    output_dir='./bert_models',  \n",
    "                    num_train_epochs=10,              # total number of training epochs\n",
    "                    per_device_train_batch_size=16,  # batch size per device during training\n",
    "                    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "                    #warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "                    weight_decay=0.01,               # strength of weight decay\n",
    "                    warmup_steps=math.ceil(len(y_bert_train) / 16),\n",
    "                    logging_dir='./logs',         # directory for storing logs\n",
    "                    logging_steps=2000,\n",
    "                    learning_rate= 5e-5,\n",
    "                    save_total_limit = 1,\n",
    "                    #load_best_model_at_end= True,\n",
    "                )\n",
    "        \n",
    "        print('Load BERT model ...')\n",
    "        \n",
    "        with training_args.strategy.scope():\n",
    "            model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "\n",
    "        trainer = TFTrainer(\n",
    "            model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "            args=training_args,                  # training arguments, defined above\n",
    "            train_dataset=train_dataset,         # training dataset\n",
    "            eval_dataset=val_dataset             # evaluation dataset\n",
    "        )\n",
    "        \n",
    "        print('Train BERT model ...')\n",
    "        trainer.train()\n",
    "        \n",
    "        print('Predict ...')\n",
    "        preds = trainer.model.predict(test_dataset.batch(8))\n",
    "        p_softmax = tf.nn.softmax(preds.logits, axis=1).numpy()\n",
    "        embeddings = sbert.encode(X_used)\n",
    "        X_test = sbert.encode(X_test)\n",
    "        y_pred = p_softmax.argmax(axis=1)\n",
    "\n",
    "        np.save(f'./res/{dataset}_{encoding}_{str(query_strategy)}_{name}{name_postfix}_{sample_size_per_step}_{al_steps}_{save_and_repeat}/y_pred_{i}', y_pred)\n",
    "        np.save(f'./res/{dataset}_{encoding}_{str(query_strategy)}_{name}{name_postfix}_{sample_size_per_step}_{al_steps}_{save_and_repeat}/y_true_{i}', y_test)\n",
    "        \n",
    "        from sklearn.metrics import f1_score\n",
    "        print(f1_score(y_test, y_pred, average='micro'), f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ded64-e55e-4cfe-a149-4b60cc7dba2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d0d262-cfe5-4683-b61d-d6c08bf1ca0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cbbd90-5912-4eef-aec9-45a5ca5d9803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb0058-e486-4888-8a25-0e527f39ac9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02a21c6-9f76-4f46-a524-4d4ab95b538d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b5f81-d587-4810-aaff-65840842b17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b6bc64-2074-40df-a819-114afcc2e027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af544631-6fe6-459c-a2fd-1e6bebb17b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33033e-c594-4a77-b14c-49be3cbdb522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd665f9-fa8b-443d-99e2-d721b6fd6ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcae5b6-ca34-40a9-8afa-c9ef346b462a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e9b8aa-c805-49c7-a8a0-62ae30c37507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e4bca2b-2b47-462c-aa34-7238651ab03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "cwd = os.getcwd()\n",
    "sys.path.append(cwd + '/../.') \n",
    "\n",
    "from src.training_loop2 import QueryStrategy\n",
    "from data.datasets import Datasets, Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "058845fb-0aec-4e90-8dc4-fb849afe9726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TFBertForSequenceClassification, TFTrainer, TFTrainingArguments, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f408007e-b673-4632-a0d7-2fa9b0302cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "name='j'\n",
    "dataset='app_store'\n",
    "encoding='sbert'\n",
    "al_steps=100\n",
    "sample_size_per_step=1\n",
    "save_and_repeat=1\n",
    "query_strategy='unc'\n",
    "random=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7600d591-320b-46e4-b08e-52f1fd1daf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load index ... \n"
     ]
    }
   ],
   "source": [
    "name_postfix = \"\"\n",
    "if random:\n",
    "    print('Random query strategy is used. Selection is ignored.')\n",
    "    name_postfix = \"_rnd\"\n",
    "    query_strategy = QueryStrategy.unc\n",
    "\n",
    "print('load index ... ')\n",
    "used_training_index_dir = f'../eval_proxy/res/{dataset}_{encoding}_{str(query_strategy)}_{name}{name_postfix}_{sample_size_per_step}_{al_steps}_{save_and_repeat}/used_training_index.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8b7d1ba-7429-4151-9633-e8f60a650b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../eval_proxy/res/app_store_sbert_unc_j_1_100_1/used_training_index.npy\n"
     ]
    }
   ],
   "source": [
    "print(used_training_index_dir)\n",
    "used_training_index = np.load(used_training_index_dir, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9242429-1c94-4cb4-9ec6-302d8cdf51b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data...\n"
     ]
    }
   ],
   "source": [
    "print('load data...')\n",
    "data_X = np.load(f'../data/datasets/{dataset}_X.npy')\n",
    "data_y = np.load(f'../data/datasets/{dataset}_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99ba1268-b631-480e-8e40-5597fe77ad0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
    "sss.get_n_splits(data_X, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "774df341-1a39-44ba-85cb-cd00d086c08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #0 ...\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for train_index, test_index in sss.split(data_X, data_y):\n",
    "    print(f'Model #{i} ...')\n",
    "    X_train, X_test = data_X[train_index], data_X[test_index]\n",
    "    y_train, y_test = data_y[train_index], data_y[test_index]\n",
    "        \n",
    "    idx = used_training_index[i][-1]\n",
    "        \n",
    "    X_used = X_train[idx]\n",
    "    y_used = y_train[idx]\n",
    "    break;\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7bc422e-b324-40f8-b145-6445e50b24f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert_train, X_bert_eval, y_bert_train, y_bert_eval = train_test_split(X_used, y_used, test_size=0.10, random_state=42, stratify=y_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6673d250-523a-425d-9fb5-057b62eaf246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize ...\n"
     ]
    }
   ],
   "source": [
    "print('tokenize ...')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd4844be-4247-4caa-a4ca-2e7480d8b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_bert_train.tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(X_bert_eval.tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3781d05d-4eae-4274-a79c-643848f51ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert_train = None\n",
    "X_bert_eval = None\n",
    "X_test = None\n",
    "tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e2fb183-cb10-4a46-b763-c10c11ca7511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 09:18:05.085959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 09:18:05.096358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 09:18:05.096864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 09:18:05.097995: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-02 09:18:05.099145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 09:18:05.099593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 09:18:05.099942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 09:18:06.005419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 09:18:06.005917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 09:18:06.006349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 09:18:06.006703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29064 MB memory:  -> device: 0, name: GRID V100S-32Q, pci bus id: 0000:06:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_bert_train\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    y_bert_eval\n",
    "))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca327660-5c34-4fd4-8686-c262641feeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings=None\n",
    "val_encodings=None\n",
    "test_encodings=None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
