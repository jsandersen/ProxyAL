{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92885ee2-f3c5-40cf-b568-d2eaf0921846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "401f8f35-40a0-4b3b-95b4-a79e7d0a6506",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set= 'app_store'\n",
    "encoding = 'sbert'\n",
    "use_random = False\n",
    "\n",
    "rng = '_rnd' if use_random else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43794bae-160b-4548-9d1a-d6586e14dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = np.load('./../data/%s_X.npy' % data_set)\n",
    "data_y = np.load('./../data/%s_y.npy' % data_set)\n",
    "\n",
    "used_index_meta_list = np.load('./../eval_proxy/%s_%s%s/used_training_index.npy' % (data_set, encoding, rng), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "241566c3-3d4e-4f82-ab26-c06f1c7f0374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
    "sss.get_n_splits(data_X, data_y)\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in sss.split(data_X, data_y):\n",
    "    X_train, X_test = data_X[train_index], data_X[test_index]\n",
    "    y_train, y_test = data_y[train_index], data_y[test_index]\n",
    "    break;\n",
    "\n",
    "X_test = X_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c553cc5-5ec6-4b09-aeb8-c450780b4d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92c06bbf-7c0e-4d38-adb8-b857879efde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pool\n",
    "i = 0\n",
    "used_index = used_index_meta_list[i][1]\n",
    "\n",
    "X_pool = []\n",
    "y_pool = []\n",
    "for i in used_index:\n",
    "    X_pool.append(X_train[i])\n",
    "    y_pool.append(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "310678ce-13cf-4a45-be52-0a070c08f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pool = X_train[:630].tolist()\n",
    "y_pool = y_train[:630].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edc4cb0c-0988-4550-ab20-8eaabcad6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50dd572a-b6df-43aa-8b8f-8c86f128938f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "630"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d2872ad-35e3-48b3-a40b-87a700a5c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = None\n",
    "y_train = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59c0bd32-afbb-4ff5-a79a-ad379f98fb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0380026-fd1b-4e7c-b00b-c5a8905a1606",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert_train, X_bert_eval, y_bert_train, y_bert_eval = train_test_split(X_pool, y_pool, test_size=0.10, random_state=42, stratify=y_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfa1a7ce-8579-4561-ac9c-6c7808b67fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(567, 63)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_bert_train), len(X_bert_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e33bd-3f1f-4101-a758-4a89dadebbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "828687e8-3b5a-4231-bcc0-757f563b9dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_encodings = tokenizer(X_bert_train, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(X_bert_eval, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c5d4ec8-9b28-4f20-be38-2b880d7eeeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-19 14:30:07.439257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-19 14:30:07.450352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-19 14:30:07.450862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-19 14:30:07.452008: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-19 14:30:07.452736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-19 14:30:07.453150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-19 14:30:07.453476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-19 14:30:12.115963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-19 14:30:12.116517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-19 14:30:12.116922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-19 14:30:12.117261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 27914 MB memory:  -> device: 0, name: GRID V100S-32Q, pci bus id: 0000:06:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_bert_train\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    y_bert_eval\n",
    "))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86a5e359-b0a2-4f70-a056-3c6301142d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings=None\n",
    "val_encodings=None\n",
    "test_encodings=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cdec727-0ec5-4e5e-bed5-ed0a9d0f3b91",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "--load_best_model_at_end requires the saving steps to be a round multiple of the evaluation steps, but found 500, which is not a round multiple of 2000.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6200/2787165717.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m training_args = TFTrainingArguments(\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mevaluation_strategy\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"steps\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./bert_models'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/training_args_tf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_on_each_node, no_cuda, seed, fp16, fp16_opt_level, fp16_backend, fp16_full_eval, local_rank, xpu_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, deepspeed, label_smoothing_factor, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, gradient_checkpointing, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, tpu_name, tpu_zone, gcp_project, poly_power, xla)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    741\u001b[0m                 )\n\u001b[1;32m    742\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_strategy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mIntervalStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPS\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_steps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_steps\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    744\u001b[0m                     \u001b[0;34m\"--load_best_model_at_end requires the saving steps to be a round multiple of the evaluation \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                     \u001b[0;34mf\"steps, but found {self.save_steps}, which is not a round multiple of {self.eval_steps}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: --load_best_model_at_end requires the saving steps to be a round multiple of the evaluation steps, but found 500, which is not a round multiple of 2000."
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "from transformers import TFBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "\n",
    "\n",
    "training_args = TFTrainingArguments(\n",
    "            evaluation_strategy= \"steps\",\n",
    "            output_dir='./bert_models',  \n",
    "            num_train_epochs=10,              # total number of training epochs\n",
    "            per_device_train_batch_size=16,  # batch size per device during training\n",
    "            per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "            #warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "            weight_decay=0.01,               # strength of weight decay\n",
    "            warmup_steps=math.ceil(len(y_bert_train) / 16),\n",
    "            logging_dir='./logs',         # directory for storing logs\n",
    "            logging_steps=2000,\n",
    "            learning_rate= 5e-5,\n",
    "            save_total_limit = 1,\n",
    "            #load_best_model_at_end= True,\n",
    "        )\n",
    "\n",
    "with training_args.strategy.scope():\n",
    "    model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "\n",
    "trainer = TFTrainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63f7f248-84ee-4ddb-b082-8cab6c1e6683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-19 14:30:30.346342: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_3\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "input: \"Placeholder/_2\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "      type: DT_INT32\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 417\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 417\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2021-10-19 14:30:30.410200: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "863328f0-76f5-4831-93e4-31bf9a04eb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  2307      \n",
      "=================================================================\n",
      "Total params: 109,484,547\n",
      "Trainable params: 109,484,547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5533d06-e879-414a-8e8b-40ea91783e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-19 14:33:29.974581: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_3\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "input: \"Placeholder/_2\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "      type: DT_INT32\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 1790\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 1790\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.model.predict(test_dataset.batch(8))\n",
    "p_softmax = tf.nn.softmax(preds.logits, axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d45a7232-9371-4625-8d06-e23677148257",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = p_softmax.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e8cc22b-8349-4fb2-b71c-20684f04dd93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8620150187734669, 0.8224995047948452)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred, average='micro'), f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e60148-70ae-447b-8e45-b77e75143778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a82c8b03-a51a-45de-a5d9-0bdc16cf80d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8563829787234043, 0.8183368300130778)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred, average='micro'), f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31cfef7-e1a5-413a-98a6-9ac8c542230f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40479e4c-2ac8-47ec-875e-65403e5165cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8504380475594493, 0.8105004485531732)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred, average='micro'), f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af505884-ed6e-4944-b3dd-b7c0484bc7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2dc50be8-0da5-4969-99d8-fac3b4d4a114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.853566958698373, 0.8173382270993904)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred, average='micro'), f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7aa2e8-a19c-401b-aeac-fd2dfac9e23b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
